# Meta-Reinforcement Learning for Business Decision-Making: Mixture Policy Optimization with Hierarchical Control
A Two-Tier DQN Framework for Real-Time Budget Allocation and Bid Adjustment in Multi-Channel Advertising
This paper applies meta-reinforcement learning to find optimal policies in mixed operational settings.
In real-world business environments, companies must allocate limited resources - such as ad budgets, sales efforts, or promotional offers - across competing strategies. Each strategy's performance (conversion rates, reach, and cost-effectiveness) can change quickly due to timing, customer behavior, and market competition. Traditional rules or fixed models often misallocate resources - overspending when costs rise or missing chances when performance improves - resulting in inefficiency.
To solve this, we introduce a two-tier meta-reinforcement learning system that separates high-level strategy from low-level actions. A meta-controller runs at regular intervals (e.g., every N minutes), using live metrics like conversion rate, cost per acquisition, and budget share to decide how to split resources across strategies. Then, individual Deep Q-Network (DQN) agents adjust actions within each strategy - boosting investment when returns are high and pulling back when costs exceed thresholds.
Each interval is treated as a mini-task. The meta-policy learns when to favor certain strategies (e.g., video ads in the evening, search ads in the morning), while the DQNs fine-tune bids using Q-learning. Experience replay and target-network updates help stabilize training, and cost violations are penalized to keep spending in check.
This hierarchical setup turns live performance data into smarter, automated decisions. It adapts quickly to market shifts, balances cost and ROI, and supports efficient, real-time operations.
